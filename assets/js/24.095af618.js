(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{383:function(a,s,e){"use strict";e.r(s);var r=e(42),t=Object(r.a)({},(function(){var a=this,s=a.$createElement,e=a._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"spark"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark"}},[a._v("¶")]),a._v(" Spark")]),a._v(" "),e("h2",{attrs:{id:"介绍"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#介绍"}},[a._v("¶")]),a._v(" 介绍")]),a._v(" "),e("p",[a._v("Spark具有如下几个主要特点：")]),a._v(" "),e("ul",[e("li",[a._v("运行速度快：Spark使用先进的DAG（Directed Acyclic Graph，有向无环图）执行引擎，以支持循环数据流与内存计算，基于内存的执行速度可比Hadoop MapReduce快上百倍，基于磁盘的执行速度也能快十倍；")]),a._v(" "),e("li",[a._v("容易使用：Spark支持使用Scala、Java、Python和R语言进行编程，简洁的API设计有助于用户轻松构建并行程序，并且可以通过Spark Shell进行交互式编程；")]),a._v(" "),e("li",[a._v("通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件，这些组件可以无缝整合在同一个应用中，足以应对复杂的计算；")]),a._v(" "),e("li",[a._v("运行模式多样：Spark可运行于独立的集群模式中，或者运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源。")])]),a._v(" "),e("h3",{attrs:{id:"spark的生态系统"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark的生态系统"}},[a._v("¶")]),a._v(" Spark的生态系统")]),a._v(" "),e("p",[a._v("主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件，各个组件的具体功能如下：")]),a._v(" "),e("ul",[e("li",[a._v("Spark Core：Spark Core包含Spark的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。Spark建立在统一的抽象RDD之上，使其可以以基本一致的方式应对不同的大数据处理场景；通常所说的Apache Spark，就是指Spark Core；")]),a._v(" "),e("li",[a._v("Spark SQL：Spark SQL允许开发人员直接处理RDD，同时也可查询Hive、HBase等外部数据源。Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行查询，并进行更复杂的数据分析；")]),a._v(" "),e("li",[a._v("Spark Streaming：Spark Streaming支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流式计算分解成一系列短小的批处理作业。Spark Streaming支持多种数据输入源，如Kafka、Flume和TCP套接字等；")]),a._v(" "),e("li",[a._v("MLlib（机器学习）：MLlib提供了常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等，降低了机器学习的门槛，开发人员只要具备一定的理论知识就能进行机器学习的工作；")]),a._v(" "),e("li",[a._v("GraphX（图计算）：GraphX是Spark中用于图计算的API，可认为是Pregel在Spark上的重写及优化，Graphx性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。")])]),a._v(" "),e("h2",{attrs:{id:"安装"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#安装"}},[a._v("¶")]),a._v(" 安装")]),a._v(" "),e("p",[a._v("首先安装 Scala。")]),a._v(" "),e("p",[a._v("先保存Scala 2.12.12，找到保存路径，进入该目录下:")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("cd  /home/hadoop/spark\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("解压文件夹：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("tar -zxvf Scala-2.12.12.tgz\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("然后配置环境变量 "),e("code",[a._v("vim /etc/profile")])]),a._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SCALA_HOME")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("/home/hadoop/spark/scala-2.12.12\n"),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v("  "),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[e("span",{pre:!0,attrs:{class:"token environment constant"}},[a._v("PATH")])]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token environment constant"}},[a._v("$PATH")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v(":")]),e("span",{pre:!0,attrs:{class:"token variable"}},[a._v("$SCALA_HOME")]),a._v("/bin\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br")])]),e("p",[a._v("使配置更改生效 "),e("code",[a._v("source /etc/profile")]),a._v("。")]),a._v(" "),e("p",[a._v("然后测试安装是否成功：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("scala -version\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("接下来安装Spark。")]),a._v(" "),e("p",[a._v("保存 spark-2.4.7-bin-without-hadoop.tgz 到/home/hadoop/spark")]),a._v(" "),e("p",[a._v("并解压:")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("tar -zxvf spark-2.4.7-bin-without-hadoop.tgz\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("修改环境变量：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("export SPARK_HOME=/home/hadoop/spark/spark-2.4.7\nexport PATH=$PATH:$SPARK_HOME/sbin\nexport PATH=$PATH:$SPARK_HOME/bin\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("p",[a._v("之后，进入/home/hadoop/spark/spark-2.4.7/conf目录下更改spark-env.sh文件："),e("code",[a._v("vim spark-env.sh")])]),a._v(" "),e("div",{staticClass:"language-shell line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SCALA_HOME")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("/home/hadoop/spark/scala-2.12.12\n"),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("JAVA_HOME")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("/usr/java/jdk1.8.0_202\n"),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("HADOOP_HOME")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("/home/hadoop/apps/hadoop-3.2.0\n"),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("HADOOP_CONF_DIR")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token variable"}},[a._v("$HADOOP_HOME")]),a._v("/etc/hadoop\n"),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("export")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SPARK_DIST_CLASSPATH")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token variable"}},[e("span",{pre:!0,attrs:{class:"token variable"}},[a._v("$(")]),a._v("/home/hadoop/apps/hadoop-3.2.0/bin/hadoop classpath"),e("span",{pre:!0,attrs:{class:"token variable"}},[a._v(")")])]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SPARK_MASTER_HOST")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("192.168")]),a._v(".188.100\n"),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SPARK_MASTER_PORT")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[a._v("7077")]),a._v("\n"),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SPARK_LOCAL_DIRS")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("/home/hadoop/spark/spark-2.4.7\n"),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[a._v("SPARK_DRIVER_MEMORY")]),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("1G\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br"),e("span",{staticClass:"line-number"},[a._v("4")]),e("br"),e("span",{staticClass:"line-number"},[a._v("5")]),e("br"),e("span",{staticClass:"line-number"},[a._v("6")]),e("br"),e("span",{staticClass:"line-number"},[a._v("7")]),e("br"),e("span",{staticClass:"line-number"},[a._v("8")]),e("br"),e("span",{staticClass:"line-number"},[a._v("9")]),e("br")])]),e("p",[a._v("修改slaves配置文件："),e("code",[a._v("vim slaves")]),a._v("，在文件里添加自己集群的ip（你的ip地址可能不同），如：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("192.168.188.100\n192.168.188.101\n192.168.188.102\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])]),e("p",[a._v("之后将自己安装好的scala和spark分别发送给另外的虚拟机（这里只发送给了101的虚拟机，如果有别的虚拟机，重复再敲一遍命令，例如把101改成102）：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("scp scala-2.12.12 hadoop101:/home/hadoop/apps\nscp spark-2.4.7 hadoop101:/home/hadoop/apps\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br")])]),e("h2",{attrs:{id:"实例"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#实例"}},[a._v("¶")]),a._v(" 实例")]),a._v(" "),e("p",[a._v("首先启动所有的集群："),e("code",[a._v("start-all.sh")])]),a._v(" "),e("p",[a._v("再查看所有节点："),e("code",[a._v("jps")])]),a._v(" "),e("p",[a._v("要想启动spark,先进入/home/hadoop/spark/spark-2.4.7/sbin目录：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v("cd  /home/hadoop/spark/spark-2.4.7/sbin\n")])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("p",[a._v("再启动："),e("code",[a._v("./start-all.sh")]),a._v("\n再查看节点："),e("code",[a._v("jps")]),a._v("\n再启动spark: spark-shell（退出spark是：:quit）")]),a._v(" "),e("Picture",{attrs:{src:"/images/hadoop/spark.png",width:"100%",alt:"spark"}}),a._v(" "),e("p",[a._v("使用 spark可以从现有的 RDD 或数据源创建 DataFrames。作为示例，我们通过 Spark 提供的 JSON 格式的数据源文件 ./examples/src/main/resources/people.json 来进行演示，该数据源内容如下：\n"),e("Picture",{attrs:{src:"/images/hadoop/people.png",width:"100%",alt:"spark"}}),a._v("\n执行如下命令导入数据源，并输出内容：")],1),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('val df=spark.read.json("file:///home/hadoop/spark/spark-2.4.7/examples/src/main/resources/people.json")\ndf.show()\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br")])]),e("Picture",{attrs:{src:"/images/hadoop/dfshow.png",width:"100%",alt:"spark"}}),a._v(" "),e("p",[a._v('DataFrames 处理结构化数据的一些基本操作,只显示 "name" 列:')]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('df.select("name").show() \n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br")])]),e("Picture",{attrs:{src:"/images/hadoop/dfselect.png",width:"90%",alt:"spark"}}),a._v(" "),e("p",[a._v('将 "age" 加 1：'),e("code",[a._v('df.select(df("name"), df("age") + 1).show()')])]),a._v(" "),e("Picture",{attrs:{src:"/images/hadoop/dfselect2.png",width:"90%",alt:"spark"}}),a._v(" "),e("p",[a._v("条件语句："),e("code",[a._v('df.filter(df("age") > 21).show()')])]),a._v(" "),e("Picture",{attrs:{src:"/images/hadoop/dffilter.png",width:"90%",alt:"spark"}}),a._v(" "),e("p",[a._v("group by 操作："),e("code",[a._v('df.groupBy("age").count().show()')])]),a._v(" "),e("Picture",{attrs:{src:"/images/hadoop/dfgroupby.png",width:"90%",alt:"spark"}}),a._v(" "),e("p",[a._v("我们也可以使用 SQL 语句来进行操作：")]),a._v(" "),e("div",{staticClass:"language- line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[a._v('df.registerTempTable("people") // 将 DataFrame 注册为临时表 people\nval result = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19") // 执行 SQL 查询\nresult.show() // 输出结果\n')])]),a._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[a._v("1")]),e("br"),e("span",{staticClass:"line-number"},[a._v("2")]),e("br"),e("span",{staticClass:"line-number"},[a._v("3")]),e("br")])])],1)}),[],!1,null,null,null);s.default=t.exports}}]);