<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hadoop I/O | Hadoop</title>
    <meta name="generator" content="VuePress 1.7.1">
    
    <meta name="description" content="Hadoop 学习笔记">
    
    <link rel="preload" href="/assets/css/0.styles.bea26999.css" as="style"><link rel="preload" href="/assets/js/app.7696aff8.js" as="script"><link rel="preload" href="/assets/js/2.3c301967.js" as="script"><link rel="preload" href="/assets/js/18.a7d27d96.js" as="script"><link rel="prefetch" href="/assets/js/10.239320ff.js"><link rel="prefetch" href="/assets/js/11.7aea8a94.js"><link rel="prefetch" href="/assets/js/12.a75c9a92.js"><link rel="prefetch" href="/assets/js/13.17c333e6.js"><link rel="prefetch" href="/assets/js/14.c658bd9d.js"><link rel="prefetch" href="/assets/js/15.adf41b6f.js"><link rel="prefetch" href="/assets/js/16.d09250f7.js"><link rel="prefetch" href="/assets/js/17.3a4cbbc8.js"><link rel="prefetch" href="/assets/js/19.3f563a7b.js"><link rel="prefetch" href="/assets/js/20.cf7aaa65.js"><link rel="prefetch" href="/assets/js/21.dabe57d5.js"><link rel="prefetch" href="/assets/js/22.8c1d8730.js"><link rel="prefetch" href="/assets/js/23.b9917a11.js"><link rel="prefetch" href="/assets/js/24.095af618.js"><link rel="prefetch" href="/assets/js/25.d294f37a.js"><link rel="prefetch" href="/assets/js/26.fc764424.js"><link rel="prefetch" href="/assets/js/27.998a5d50.js"><link rel="prefetch" href="/assets/js/28.7d6e20a5.js"><link rel="prefetch" href="/assets/js/29.d2921661.js"><link rel="prefetch" href="/assets/js/3.566e47d9.js"><link rel="prefetch" href="/assets/js/4.23282b51.js"><link rel="prefetch" href="/assets/js/5.c842d990.js"><link rel="prefetch" href="/assets/js/6.ef414eb3.js"><link rel="prefetch" href="/assets/js/7.3152689a.js"><link rel="prefetch" href="/assets/js/8.530be27e.js"><link rel="prefetch" href="/assets/js/9.a4f965b7.js">
    <link rel="stylesheet" href="/assets/css/0.styles.bea26999.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Hadoop</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/hadoop/install.html" class="nav-link">
  Hadoop
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/hadoop/install.html" class="nav-link">
  Hadoop
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>基础</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/hadoop/install.html" class="sidebar-link">安装</a></li><li><a href="/hadoop/benchmark.html" class="sidebar-link">集群配置与管理</a></li><li><a href="/hadoop/hdfs.html" class="sidebar-link">HDFS</a></li><li><a href="/hadoop/io.html" aria-current="page" class="active sidebar-link">Hadoop I/O</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hadoop/io.html#_1-数据完整性" class="sidebar-link">1. 数据完整性</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hadoop/io.html#_1-1-hdfs的数据完整性" class="sidebar-link">1.1 HDFS的数据完整性</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_1-2-localfilesystem" class="sidebar-link">1.2 LocalFileSystem</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_1-3-checksumfilesystem" class="sidebar-link">1.3 ChecksumFileSystem</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_2-压缩" class="sidebar-link">2. 压缩</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_2-1-codec" class="sidebar-link">2.1 codec</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_2-2-压缩和输入分片" class="sidebar-link">2.2 压缩和输入分片</a></li></ul></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-序列化" class="sidebar-link">3. 序列化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-1-writable" class="sidebar-link">3.1 Writable</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-2-writable类" class="sidebar-link">3.2 Writable类</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-2-2-text类型" class="sidebar-link">3.2.2 Text类型</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-2-3-byteswritable" class="sidebar-link">3.2.3 BytesWritable</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-2-4-nullwritable" class="sidebar-link">3.2.4 NullWritable</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-2-5-objectwritable和genericwritable" class="sidebar-link">3.2.5 ObjectWritable和GenericWritable</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-2-6-writable集合类" class="sidebar-link">3.2.6 Writable集合类</a></li></ul></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-3-序列化框架" class="sidebar-link">3.3 序列化框架</a></li><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-4-基于文件的数据结构" class="sidebar-link">3.4 基于文件的数据结构</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/hadoop/io.html#_3-4-1-关于sequencefile" class="sidebar-link">3.4.1 关于SequenceFile</a></li></ul></li></ul></li><li><a href="/hadoop/mapreduce.html" class="sidebar-link">MapReduce</a></li><li><a href="/hadoop/mysql.html" class="sidebar-link">MySQL</a></li><li><a href="/hadoop/sqoop.html" class="sidebar-link">Sqoop</a></li><li><a href="/hadoop/hive.html" class="sidebar-link">Hive</a></li><li><a href="/hadoop/hbase.html" class="sidebar-link">HBase</a></li><li><a href="/hadoop/pig.html" class="sidebar-link">Pig</a></li><li><a href="/hadoop/spark.html" class="sidebar-link">Spark</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="hadoop-i-o"><a href="#hadoop-i-o" class="header-anchor">¶</a> Hadoop I/O</h1> <h2 id="_1-数据完整性"><a href="#_1-数据完整性" class="header-anchor">¶</a> 1. 数据完整性</h2> <p>尽管磁盘或网络上的每个I/O操作不太可能将错误引入自己正在读/写的数据中，但是如果系统中需要处理的数据量大到Hadoop的处理极限时，数据被损坏的概率还是很高的。</p> <p>检测数据是否损坏的常见措施时在数据第一次引入系统时计算校验和(checksum)，并在数据通过一个不可靠的通道进行传输时再次计算校验和，这样就能发现数据是否损坏。如果计算所得的新校验和与原来的校验和不匹配，我们就认为数据已损坏。</p> <p>计算校验和只能检测数据是否错误而不能修复数据。</p> <p>常用的错误检测码时CRC-32(32位循环冗余校验)，任何大小的数据输入均计算得到一个32位得整数校验和。Hadoop ChecksumFileSystem 使用CRC-32计算校验和，HDFS用于校验和计算的则是一个更有效的变体CRC-32C。</p> <h3 id="_1-1-hdfs的数据完整性"><a href="#_1-1-hdfs的数据完整性" class="header-anchor">¶</a> 1.1 HDFS的数据完整性</h3> <p>HDFS会对写入的所有数据计算校验和，并在读取数据时验证校验和。它针对每个由dfs.bytes-per-checksum指定字节的数据计算校验和。默认情况下为512个字节，由于CRC-32校验和是4个字节，所以存储校验和的额外开销低于1%。</p> <p>datanode负责在收到数据后存储该数据及其校验和之前对数据进行验证。它在收到客户端的数据或复制其他datanode的数据时执行这个操作。正在写数据的客户端将数据及其校验和发送到一系列datanode组成的管线，管线中最后一个datanode负责验证校验和。如果datanode检测到错误，客户端便会收到一个IOEXception异常的一个子类，对于该异常应以应用程序特定的方式来处理，比如重试这个操作。</p> <p>客户端从datanode读取数据时，也会验证校验和，将他们与datanode存储的校验和进行比较。每个datanode均持久保存有一个用于验证的校验和日志(persistent log of checksum verification)，所以它知道每个数据块的最后一次验证时间。客户端成功验证一个数据块后，会告诉这个datanode，datanode由此更新日志。保存这些统计信息对于检测损坏的磁盘很有价值。</p> <p>不只是客户端在读取数据块时会验证校验和，每个datanode也会在一个后台线程中运行一个DataBlockScanner，从而定期验证存储在这个datanode上的所有数据块。该项措施是解决物理存储媒体上位损坏的有力措施。</p> <p>由于HDFS存储着每个数据块的复本(replica)，因此它可以通过数据复本来修复损坏的数据块，进而得到一个新的、完好无损的复本。其基本思路是，客户端在读取数据块时，如果检测到错误，首先向namenode报告已损坏的数据块及其正在尝试读取操作的这个datanode，再抛出ChecksumException异常。namenode将这个数据块复本标记为已损坏，这样它不再将客户端处理请求直接发送到这个节点，或尝试将这个复本复制到另一个datanode。之后，它安排这个数据块的一个副本复制到另一个datanode，如此一来，数据块的复本因子(replication factor)又回到期望水平。此后，已损坏的数据块复本便被删除。</p> <p>在使用open()方法读取文件之前，将false值传递给FileSystem对象的setVerifyChecksum()方法，即可以警用校验和验证。如果在命令解释器中使用带-get选项的-ignoreCrc命令或者使用等价的-copyToLocal命令，也可以达到同样的效果。如果有一个已损坏的文件需要检查并决定如何处理，这个特性是非常有用的。例如，也许你希望在删除该文件之前尝试看看是否能够恢复部分数据。</p> <p>可以用hadoop命令的 <em>fs -checksum</em> 来检查一个文件的校验和。这可用于在HDFS中检验两个文件是否具有相同内容，<em>distcp</em> 命令也具有类似的功能。</p> <h3 id="_1-2-localfilesystem"><a href="#_1-2-localfilesystem" class="header-anchor">¶</a> 1.2 LocalFileSystem</h3> <p>Hadoop的LocalFileSystem执行客户端的校验和验证。这意味着在你写入一个名为<em>filename</em>的文件时，文件系统客户端会明确在包含每个文件块校验和的同一个目录内新建一个*.filename.crc<em>隐藏文件。文件块的大小由属性file.bytes-per-checksum控制，默认为512个字节。文件块的大小作为元数据存储在</em>.crc*文件中，所以即使文件块大小的设置已经发生变化，任然可以正确读回文件。在读取文件时需要验证校验和，并且如果检测到错误，LocalFileSystem还会抛出一个CheckSumException异常。</p> <p>校验和的计算代价时相当低的（在Java中，它们使用本地代码实现的），一般只是增加少许额外的读写/文件时间。对大多数应用来说，付出这样的额外开销以保证数据完整性是可以接受的。此外，我们也可以禁用校验和计算，特别是在底层文件系统本身就支持校验和的时候。在这种情况下，使用RawLocalFileSystem替代LocalFileSystem。要想在一个应用中实现全局校验和验证，需要将<em>fs.file.impl</em>属性设置为<em>org.apache.hadoop. fs.RawLocalFileSystem</em>进而实现对文件URI的重新映射。还有一个可选方案可以直接新建一个RawLocalFileSystem实例。如果想针对一些读操作禁用校验和，这个方案非常有用。示例如下：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token class-name">Configuration</span> conf <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token class-name">FileSystem</span> fs <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">RawLocalFileSystem</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
fs<span class="token punctuation">.</span><span class="token function">initialize</span><span class="token punctuation">(</span><span class="token keyword">null</span><span class="token punctuation">,</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="_1-3-checksumfilesystem"><a href="#_1-3-checksumfilesystem" class="header-anchor">¶</a> 1.3 ChecksumFileSystem</h3> <p>LocalFileSystem通过ChecksumFileSystem来完成自己的任务，有了这个类，向其他文件系统（无校验和系统）加入校验和就非常简单，因为ChecksumFileSystem类继承自FileSystem类。一般用法如下：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token class-name">FileSystem</span> rawFs <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token class-name">FileSystem</span> checksummedFs <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ChecksumFileSystem</span><span class="token punctuation">(</span>rawFs<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>底层文件系统称为“源”(raw)文件系统，可以使用ChecksumFileSystem实例的getRawFileSystem()方法获取它。ChecksumFileSystem类还有一些与校验和有关的有用方法，比如getChecksumFile()可以获得任意一个文件的校验和文件路径。其它更多方法可以查看参考文档。</p> <p>如果ChecksumFileSystem类在读取文件时检测到错误，回到用自己的reportChecksumFailure()方法。默认实现为空方法，但LocalFileSystem类会将这个出错的文件及其校验和移到同一个存储设备上的名为bad_files的边际文件夹(side directory)中。管理员应该定期检查这些坏文件并采取相应的行动。</p> <h3 id="_2-压缩"><a href="#_2-压缩" class="header-anchor">¶</a> 2. 压缩</h3> <p>文件压缩有两大好处：减少存储文件所需得磁盘空间，并加速数据在网络和磁盘上得传输。这两大好处在处理大量数据时相当重要，所以我们值得仔细考虑在Hadoop中文件压缩得方法。</p> <p>有很多种不同的压缩格式、工具和算法，它们各有千秋。下表列出了与Hadoop结合使用得常见压缩方法。</p> <p>表2.1 压缩格式总结</p> <p><img src="/images/hadoop/%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E6%80%BB%E7%BB%93.jpg" alt="压缩格式总结.jpg"></p> <p>所有压缩算法都需要权衡空间/时间：压缩和解压缩速度更快，其代价通常是只能节省少量得空间。表2.1列出的所有压缩工具都提供了9个不同的选项来控制压缩时必须考虑的权衡：选项-1为优化压缩速度，-9为优化压缩空间。例如，下述命令通过最快的压缩方法创建一个名为<em>file.gz</em>的压缩文件：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">gzip</span> -1 <span class="token function">file</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>不同压缩工具有不同的压缩特性。gzip是一个通用的压缩工具，在空间/时间性能的权衡中，居于其他两个压缩方法之间。bizp2的压缩能力强于gzip，但压缩速度更慢一些。尽管bzip2的解压速度比压缩速度快，但比其它压缩格式要慢一些。另一方面，LZO、LZ4和Snappy均优化压缩速度，其速度比gzip快一个数量级，但压缩效率略逊一筹。Snappy和LZ4的解压速度比LZO高出很多。</p> <p>上表中的“是否可切分”列表示对应的压缩算法是否支持切分(splitable)，也就是说，是否可以搜索数据流的任意位置并进一步往下读取数据。可切分压缩格式尤其适合MapReduce。</p> <h3 id="_2-1-codec"><a href="#_2-1-codec" class="header-anchor">¶</a> 2.1 codec</h3> <p><em>Codec</em>是压缩-解压缩算法的一种实现。在Hadoop中，一个对CompressionCodec接口的实现代表一个codec。例如，GzipCodec包装了gzip的压缩和解压缩算法.表2.1列举了Hadoop实现的codec。</p> <p>表2.1 Hadoop的压缩codec</p> <p><img src="/images/hadoop/Hadoop%E7%9A%84%E5%8E%8B%E7%BC%A9codec.jpg" alt="Hadoop的压缩codec.jpg"></p> <p>LZO代码库拥有GPL许可，因而可能没有包含在Apache的发行版本中，因此，Hadoop的codec需要从Google(http://code.google.com/p/hadoop-gpl-compression)或GitHub(http://github.com/kevinwei/hadoop-lzo)下载，该代码库包含有修正的软件错误及其他一些工具。LzopCoder与lzop工具兼容，LzopCodec基本上是LZO格式的但包含额外的文件头，因此这通常就是你想要的。也有针对纯LZO格式的LzoCodec，并且使用.lzo_deflate作为文件扩展名（类似于DEFLATE，是gzip格式但不包含文件头）</p> <h4 id="_2-1-1-通过compressioncodec对数据流进行压缩和解压缩"><a href="#_2-1-1-通过compressioncodec对数据流进行压缩和解压缩" class="header-anchor">¶</a> 2.1.1 通过CompressionCodec对数据流进行压缩和解压缩</h4> <p>CompressionCodec包含两个函数，可以轻松用于压缩和解压缩数据。如果对写入输出数据流的数据进行压缩，可用createOutputStream(OutputStream out)方法在底层的数据流中对需要以压缩格式写入在此之前尚未压缩的数据新建一个CompressionOutputStream对象。相反，对输入数据流中读取的数据进行解压缩的时候，则调用createInputStream(InputStream in)获取CompressionInputStream，可以通过该方法从底层数据流读取解压缩后的数据。</p> <p>CompressionOutputStream和CompressionInputStream，类似于java.util.zip.DeflateOutputStream和java.util.zip.DeflateInputStream，子不过前两者能够重置其底层的压缩或解压缩方法，对于某些将部分数据流(section of data stream)压缩为单独数据块(block)的应用，例如SequenceFile，这个能力是非常重要的。</p> <p>范例2.1 该程序压缩从标准输入读取的数据，然后将其写到标准输出</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">StreamCompressor</span><span class="token punctuation">{</span>
	<span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span><span class="token punctuation">{</span>
    	<span class="token class-name">String</span> codecClassname <span class="token operator">=</span> arg<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token class-name">Class</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token operator">?</span><span class="token punctuation">&gt;</span></span> codecClass <span class="token operator">=</span> <span class="token class-name">Class</span><span class="token punctuation">.</span><span class="token function">forName</span><span class="token punctuation">(</span>codecClassname<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">Configuration</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">CompressionCodec</span> codec <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token class-name">CompressionCodec</span><span class="token punctuation">)</span> <span class="token class-name">ReflectionUtils</span><span class="token punctuation">.</span><span class="token function">newInstance</span><span class="token punctuation">(</span>codecClass<span class="token punctuation">,</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
        
        <span class="token class-name">CompressionOutputStream</span> out <span class="token operator">=</span> codec<span class="token punctuation">.</span><span class="token function">createOutputStream</span><span class="token punctuation">(</span><span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">IOUtils</span><span class="token punctuation">.</span><span class="token function">copyBytes</span><span class="token punctuation">(</span><span class="token class-name">System</span><span class="token punctuation">.</span>in<span class="token punctuation">,</span>out<span class="token punctuation">,</span><span class="token number">4096</span><span class="token punctuation">,</span><span class="token boolean">false</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        out<span class="token punctuation">.</span><span class="token function">finish</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>该应用希望将符合CompressionCodec实现的完全合格名称作为第一个命令行参数。我们使用ReflectionUtils新建一个codec实例，并由此获得在System.out上支持压缩的一个包裹方法。然后对IOUtils对象调用copyBytes()方法将输入数据复制到输出，（输出由CompressionOutputStream对象压缩）。最后，我们对CompressionOutputStream对象调用finish()方法，要求压缩方法完成到压缩数据流的写操作，但不关闭这个数据流。我们可以通过下面这行命令做一个测试，通过GzipCodec的StreamCompressor对象对字符串&quot;Text&quot;进行压缩，然后使用<em>gunzip</em>从标准输入中对它进行读取并解压缩操作：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>% <span class="token builtin class-name">echo</span> <span class="token string">&quot;Text&quot;</span> <span class="token operator">|</span> hadoop StreamCompressor org.apache.hadoop.io compress.GzipCodec <span class="token operator">|</span> gunzip
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="_2-2-压缩和输入分片"><a href="#_2-2-压缩和输入分片" class="header-anchor">¶</a> 2.2 压缩和输入分片</h3> <p>在考虑如何压缩将由MapReduce处理的数据时，理解这些压缩格式是否支持切分(splitting)是非常重要的。以一个存储在HDFS文件系统中且压缩前大小为1GB的文件为例。如果HDFS的块大小设置为128MB，那么该文件将被存储在8个块中，把这个文件作为输入数据的MapReduce作业，将创建8个输入分片，其中每个分片作为一个单独的map任务的输入被独立处理。</p> <p>现在想象一下，文件时经过gzip压缩的，且压缩文件大小为1GB。与以前一样，HDFS将这个文件保存为8个数据快。但是，将每个数据块单独作为一个输入分片时无法实现工作的，因为无法实现从gzip压缩数据流的任意位置读取数据，所以让map任务独立于其他任务进行数据读取是行不通的。gzip格式使用DEFLATE算法来存储压缩后的数据，而DEFLATE算法将数据存储在一系列连续的压缩块中。问题在于每个块的其实位置并没有以任何形式标记，所以在读取时无法从数据流的任意当前位置前进到下一块的起始位置读取下一个数据块，从而实现与整个数据流的同步。由于上述原因，gzip并不支持文件切分。</p> <p>在这种情况下，MapReduce会采用正确的做法，它不会尝试切分gzip压缩文件，因为它知道输入时gzip压缩文件（通过文件扩展名看出）且gzip不支持切分。这是可行的，但牺牲了数据的本地性：一个map任务处理8个HDFS块，而其中大多数块并没有存储在执行该map任务的节点上。而且，map任务数越少，作业的粒度就较大，因而运行的时间可能会更长。</p> <p>在前面假设的例子中，如果文件是通过LZO压缩的，我们会面临相同的问题，因为这个压缩格式也不支持数据读取和数据流同步。但是，在预处理LZO文件的时候使用包含在Hadoop LZO库文件中的索引工具是可能的。</p> <p>另一方面，bzip2文件提供不同数据块之间的同步表示（pi的48位近似值），因而它支持切分。</p> <h2 id="_3-序列化"><a href="#_3-序列化" class="header-anchor">¶</a> 3. 序列化</h2> <p>序列化(serialization)是指将结构化对象转化位字节流以便于在网络上传输或写到磁盘进行永久存储的过程。反序列化(deserialization)是指将字节流转回结构化对象的逆过程。</p> <p>序列化用于分布式数据处理的两大领域：进程间通信和永久存储</p> <p>在Hadoop中，系统中多个节点上进程间的通信是通过“远程过程调用”(RPC,remote procedure call)实现的。RPC协议将消息序列化成二进制流后发送到远程节点，远程节点接着将二进制流反序列化为原始消息。通常情况下，RPC序列化格式如下：</p> <ul><li>紧凑</li></ul> <p>紧凑格式能够充分利用网络带宽（数据中心中最稀缺的资源）。</p> <ul><li>快速</li></ul> <p>进程间通信形成了分布式系统的股价，所以需要尽量减少序列化和反序列化的性能开销，这是最基本的。</p> <ul><li>可扩展</li></ul> <p>为了满足新的需求，协议不断变化。所以在控制客户端和服务器的过程中，需要直接引进相应的协议。例如，需要能够在方法调用的过程中增添新的参数，并且新的服务器需要能够接受来自老客户端的老格式的消息（无新增的参数）</p> <ul><li>支持互操作</li></ul> <p>对于某些系统来说，希望能支持以不同语言写的客户端与服务器交互，所以需要设计一种特定的格式来满足这一需求。</p> <p>表面看来，序列化框架对选择用于数据持久存储的数据格式应该会有不同的要求。毕竟RPC的存活时间不到1秒钟，持久存储的数据却可能在写到磁盘若干年后才会被读取。但结果是，RPC序列化格式的四大理想属性对持久存储格式而言也很重要。我们希望存储格式比较紧凑（进而高效使用存储空间）、快速（读/写数据的额外开销比较小）、可扩展（可以透明地读取老格式的数据）且可以互操作（以可以使用不同的语言读/写永久存储的数据）。</p> <p>Hadoop使用的是自己的序列化格式Writable，它绝对紧凑、速度快，但不太容易用Java以外的语言进行扩展或使用。因为Writable是Hadoop的核心（大多数MapReduce程序都会为键和值类型使用它）。</p> <h3 id="_3-1-writable"><a href="#_3-1-writable" class="header-anchor">¶</a> 3.1 Writable</h3> <p>Writable接口定义了两个方法：一个将其状态写入DataOutput二进制流，另一个从DataInput二进制流读取状态：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">package</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io</span><span class="token punctuation">;</span>

<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">DataOutput</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">DataInput</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>io<span class="token punctuation">.</span></span><span class="token class-name">IOException</span><span class="token punctuation">;</span>

<span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">Writable</span><span class="token punctuation">{</span>
	<span class="token keyword">void</span> <span class="token function">write</span><span class="token punctuation">(</span><span class="token class-name">DataOutput</span> out<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">;</span>
    <span class="token keyword">void</span> <span class="token function">readFields</span><span class="token punctuation">(</span><span class="token class-name">DataInput</span> in<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><h6 id="writablecomparable接口和comparator"><a href="#writablecomparable接口和comparator" class="header-anchor">¶</a> WritableComparable接口和comparator</h6> <p>IntWritable实现原始的WritableComparable接口，该接口继承自Writable和java.lang.Comparable接口：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">package</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io</span><span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">WritableComparable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">&gt;</span></span> <span class="token keyword">extends</span> <span class="token class-name">Writable</span><span class="token punctuation">,</span><span class="token class-name">Comparable</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">&gt;</span></span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>对MapReduce来说，类型比较非常重要，因为中间有个基于键的排序阶段。Hadoop提供的一个优化接口是继承自Java Comparator 的RawComparator接口：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">package</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io</span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token namespace">java<span class="token punctuation">.</span>util<span class="token punctuation">.</span></span><span class="token class-name">Comparator</span><span class="token punctuation">;</span>
<span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">RawComparator</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">&gt;</span></span> <span class="token keyword">extends</span> <span class="token class-name">Comparator</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">T</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">{</span>
 	<span class="token keyword">public</span> <span class="token keyword">int</span> <span class="token function">compare</span><span class="token punctuation">(</span><span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> b1<span class="token punctuation">,</span><span class="token keyword">int</span> s1<span class="token punctuation">,</span><span class="token keyword">int</span> l1<span class="token punctuation">,</span><span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> b2<span class="token punctuation">,</span><span class="token keyword">int</span> s2<span class="token punctuation">,</span>intl2<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>该接口允许其实现直接比较数据流中的记录，无须先把数据流反序列化位对象，这样避免了新建对象的额外开销。</p> <h3 id="_3-2-writable类"><a href="#_3-2-writable类" class="header-anchor">¶</a> 3.2 Writable类</h3> <p>Hadoop自带的org.apache.hadoop.io包中有广泛的Writable类可供选择。它们的层次结构如图3.1所示。</p> <p>图3.1 Writable类的层次结构</p> <p><img src="/images/hadoop/Writable%E7%B1%BB%E7%9A%84%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84.jpg" alt="Writable类的层次结构.jpg"></p> <h4 id="_3-2-1-java基本类型的writable封装器"><a href="#_3-2-1-java基本类型的writable封装器" class="header-anchor">¶</a> 3.2.1 Java基本类型的Writable封装器</h4> <p>Writable类对所有Java基本类型（参见表3.1）提供封装，char类型除外（可以存储在IntWritable中）。所有的封装包含get()和set()两个方法用于读取或存储封装的值。</p> <p>表3.1 Java基本类型的Wriable表</p> <p><img src="/images/hadoop/Java%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E7%9A%84Writable.jpg" alt="Java基本类型的Writable.jpg"></p> <p>对整数进行编码时，有两种选择，既定长格式(IntWritable和LongWritable)和变长格式(VIntWritable和VLongWritable)。</p> <h3 id="_3-2-2-text类型"><a href="#_3-2-2-text类型" class="header-anchor">¶</a> 3.2.2 Text类型</h3> <p>Text是针对UTF-8序列的Writable类。一般可以认为它是java.lang.String的Writable等价。</p> <p>Text类使用整型（通过边长编码的方式）来存储字符串编码中所需的字节数，因此该最大值为2GB。另外，Text使用标准UTF-8编码，这使得它能够更简便地与其他理解UTF-8编码的工具进行交互操作。</p> <h3 id="_3-2-3-byteswritable"><a href="#_3-2-3-byteswritable" class="header-anchor">¶</a> 3.2.3 BytesWritable</h3> <p>BytesWritable是对二进制数据数组的封装。它的序列化格式为一个指定所含数据字节数的整数域（4字节），后跟数据内容本身。例如长度为2的字节数组包含数值3和5，序列化形式为一个4字节的整数（00000002）和该数组中的两个字节（03和05）：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token class-name">BytesWritable</span> b <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">BytesWritable</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">{</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> bytes <span class="token operator">=</span> <span class="token function">serialize</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span><span class="token class-name">StringUtils</span><span class="token punctuation">.</span><span class="token function">byteToHexString</span><span class="token punctuation">(</span>bytes<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token string">&quot;000000020305&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>BytesWritable是可变的，其值可以通过set()方法进行修改。和Text相似，BytesWritable类的getBytes()方法返回的字节数组长度（容量）可能无法体现BytesWritable所存储数据的实际大小。可以通过getLength()方法来确定BytesWritable的大小。示例如下：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>b<span class="token punctuation">.</span><span class="token function">setCapacity</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token function">getLength</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token function">getBytes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>length<span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h3 id="_3-2-4-nullwritable"><a href="#_3-2-4-nullwritable" class="header-anchor">¶</a> 3.2.4 NullWritable</h3> <p>NullWritable是Writable的特殊类型，它的序列化长度为0。它并不从数据流中读取数据，也不写入数据。它充当占位符；例如，在MapReduce中，如果不需要使用键或值得序列化地址，就可以将键或值声明为NullWritable,这样可以高效存储常量控制。如果希望存储一系列数值，与键-值对相对，NullWritable也可以用作在SequenceFile中的键。它是一个不可变的单实例类型，通过调用NullWritable.get()方法可以获取这个实例。</p> <h3 id="_3-2-5-objectwritable和genericwritable"><a href="#_3-2-5-objectwritable和genericwritable" class="header-anchor">¶</a> 3.2.5 ObjectWritable和GenericWritable</h3> <p>ObjectWritable是对Java基本类型(String,enum,Writable,null或这些类型组成的数组)的一个通用封装。它在Hadoop RPC中用于对方法的参数和返回类型进行分装和解封装。</p> <p>当一个字段中包含多个类型时，ObjectWritable非常有用：例如，如果SequenceFile中的值包含多个类型，就可以将值声明为ObjectWritable，并将每个类型封装在一个ObjectWritable中。作为一个通用的机制，每次序列化都写封装类型的名称，这非常浪费空间。如果封装的类型数量比较少并且能够提前知道，那么可以通过使用静态类型的数组，并使用对序列化后的类型的引用加入位置索引来提高性能。GenericWritable类采取的就是这种方式，所以你得在继承的子类中指定支持什么类型。</p> <h3 id="_3-2-6-writable集合类"><a href="#_3-2-6-writable集合类" class="header-anchor">¶</a> 3.2.6 Writable集合类</h3> <p>org.apache.hadoop.io软件包中有6个Writable集合类，分别是ArrayWritable、ArrayPrimitiveWritable、TwoDArrayWritable、MapWritable、SortedMapWritable以及EnumMapWritable。</p> <h2 id="_3-3-序列化框架"><a href="#_3-3-序列化框架" class="header-anchor">¶</a> 3.3 序列化框架</h2> <p>尽管大多数Mapreduce程序使用的都是Writable类型的键和值，但这并不是MapReduce API强制使用的。事实上，可以使用任何类型，只要能有一种机制对每个类型进行类型与二进制表示的来回转换就可以。</p> <p>为了支持这一机制，Hadoop有一个针对可替换序列化康佳(serialization framework)的API。序列化框架用一个Serialization实现（包含在org.apache.hadoop.io.serializer包中）来表示。例如，WritableSerialization类是对Writable类型的Serialization实现。Serialization对象定义了从类型到Serializer实例（将对象转换为字节流）和Deserializer实例（将字节流转换为对象）的映射方式。</p> <p>为了注册Serialization实现，需要将io.serizalizations属性设置为一个由逗号分隔的类名列表。它的默认值包括org.apache.hadoop.io.serializer.WritableSerialization和Avro指定(Specific)序列化及Reflect（自反）序列化类，这意味着只有Writable对象和Avro对象才可以在外部序列化和反序列化。</p> <p>Hadoop包含一个名为JavaSerialization的类，该类使用Java Object Serialization。尽管它方便了我们在MapReduce程序中使用便准的Java类型，如Integer或String，但不如Writable高效，所以不建议使用。</p> <h2 id="_3-4-基于文件的数据结构"><a href="#_3-4-基于文件的数据结构" class="header-anchor">¶</a> 3.4 基于文件的数据结构</h2> <p>对于某些应用，我们需要一种特殊的数据结构来存储自己的数据。对于基于MapReduce的数据处理，将每个二进制数据大对象(blob)单独放在各自的文件中不可能实现扩展性，所以Hadoop为此开发了很多更高层次的容器。</p> <h3 id="_3-4-1-关于sequencefile"><a href="#_3-4-1-关于sequencefile" class="header-anchor">¶</a> 3.4.1 关于SequenceFile</h3> <p>考虑日志文件，其中每一行文本代表一条日志记录。纯文本不适合记录二进制类型的数据。在这种情况下，Hadoop的SequenceFile类非常适合，为二进制键-值对提供了一个持久数据结构。将它作为日志文件的存储格式时，你可以自己选择键（比如LongWritable类型所表示的时间戳），以及值可以是Writable类型（用于表示日志记录的数量）。</p> <p>SequenceFiles也可以作为小文件的容器。HDFS和MapReduce时针对大文件优化的，所以通过SequenceFile类型将小文件包装起来，可以获得更高效率的存储和处理。</p> <p>####3.4.1 SequenceFile的写操作</p> <p>通过createWriter()静态方法可以创建SequenceFile对象，并返回SequenceFile.Writer实例。该静态方法有多个重载版本，但都需要制定带写入的数据流（FSDataOutputStream或FileSystem对象和Path对象），Configuration对象以及键和值的类型。另外，可选参数包括压缩类型以及相应的codec，Progressable回调函数用于通知写入的进度，以及在SequenceFile头文件中存储的Metadata实例。</p> <p>存储在SequenceFile中的键和值并不一定需要是Writable类型。只要能被Serialization序列化和反序列化，任何类型都可以。</p> <p>一旦拥有SequenceFile.Writer实例，就可以通过append()方法在文件末尾附加键-值对。写完后可以调用close()方法(SequenceFile.Writer实现了java.io.Closeable接口)。</p> <p>范例3.1 写入SequenceFile对象</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">SequenceFileWriteDemo</span> <span class="token punctuation">{</span>
    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> DATA <span class="token operator">=</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;One,two,buckle my shoe&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;Three,four,shut the door&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;Five,six,pick up sticks&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;Nine,ten,a big fat hen&quot;</span>
    <span class="token punctuation">}</span><span class="token punctuation">;</span>
    
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span> <span class="token punctuation">{</span>
        <span class="token class-name">String</span> uri <span class="token operator">=</span> <span class="token string">&quot;hdfs://hadoop100:9000/test/SequenceFileWriteDemo.txt&quot;</span><span class="token punctuation">;</span>
        <span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">&quot;HADOOP_USER_NAME&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;carrot&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">Configuration</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">FileSystem</span> fs <span class="token operator">=</span> <span class="token class-name">FileSystem</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>URI<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span>uri<span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">Path</span> path <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>uri<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">IntWritable</span> key <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">IntWritable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">Text</span> value <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">SequenceFile</span><span class="token punctuation">.</span><span class="token class-name">Writer</span> writer <span class="token operator">=</span> <span class="token keyword">null</span><span class="token punctuation">;</span>
        <span class="token keyword">try</span> <span class="token punctuation">{</span>
            writer <span class="token operator">=</span> <span class="token class-name">SequenceFile</span><span class="token punctuation">.</span><span class="token function">createWriter</span><span class="token punctuation">(</span>fs<span class="token punctuation">,</span> conf<span class="token punctuation">,</span> path<span class="token punctuation">,</span> key<span class="token punctuation">.</span><span class="token function">getClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token punctuation">.</span><span class="token function">getClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
                key<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token number">100</span> <span class="token operator">-</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
                value<span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span>DATA<span class="token punctuation">[</span>i <span class="token operator">%</span> DATA<span class="token punctuation">.</span>length<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;[%s]\t%s\t%s\n&quot;</span><span class="token punctuation">,</span>writer<span class="token punctuation">.</span><span class="token function">getLength</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token punctuation">;</span>
                writer<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">IOException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>
            e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span> <span class="token keyword">finally</span> <span class="token punctuation">{</span>
            <span class="token class-name">IOUtils</span><span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>writer<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><p>运行的部分结果如下图所示：</p> <p><img src="/images/hadoop/SequenceFileWriteDemo%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C.jpg" alt="SequenceFileWriteDemo运行结果.jpg"></p> <h4 id="_3-4-2-sequencefile的读操作"><a href="#_3-4-2-sequencefile的读操作" class="header-anchor">¶</a> 3.4.2 SequenceFile的读操作</h4> <p>从头到尾读取顺序文件不外乎创建SequenceFile.Reader实例后反复调用next()方法迭代读取记录。读取的是哪条记录与你使用的序列化框架相关。如果使用的是Writable类型，那么通过键和值作为参数的next()方法可以将数据流中的下一条键-值对读入变量中。</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>pulic <span class="token keyword">boolean</span> <span class="token function">next</span><span class="token punctuation">(</span><span class="token class-name">Writable</span> key<span class="token punctuation">,</span><span class="token class-name">Writable</span> val<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>如果键-值对成功读取，则返回true，如果已读到文件末尾，则返回false。</p> <p>对于其他非Writable类型的序列化框架（比如Apache Thrift），则应该使用下面两个方法：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token class-name">Object</span> <span class="token function">next</span><span class="token punctuation">(</span><span class="token class-name">Object</span> key<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span>
<span class="token keyword">public</span> <span class="token class-name">Object</span> <span class="token function">getCurrentValue</span><span class="token punctuation">(</span><span class="token class-name">Object</span> val<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在这种情况下，需要确保io.serializations属性已经设置了你想使用的序列化框架。</p> <p>如果next()返回的是非null对象，则可以从数据流中读取键-值对、并且可以通过getCurrentValue()方法读取该值。否则，如果next()返回null值，则表示已经读到文件末尾。</p> <p>范例3.2中的程序显示了如何读取包含Writable类型键-值对的顺序文件。注意如何通过调用getKeyClass()方法和getValueClass()方法进而发现SequenceFile中所使用的类型，然后通过ReflectionUtils对象生成常见键和值的实例。通过这个技术，该程序可用于处理由Writable类型键-值对的任意一个顺序文件。</p> <p>范例3.2 读取SequenceFile</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">SequenceFileReadDemo</span> <span class="token punctuation">{</span>
    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">IOException</span> <span class="token punctuation">{</span>
        <span class="token class-name">String</span> uri <span class="token operator">=</span> <span class="token string">&quot;hdfs://hadoop100:9000/test/SequenceFileWriteDemo.txt&quot;</span><span class="token punctuation">;</span>
        <span class="token class-name">Configuration</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">FileSystem</span> fs <span class="token operator">=</span> <span class="token class-name">FileSystem</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span>URI<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span>uri<span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">Path</span> path <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>uri<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token class-name">SequenceFile</span><span class="token punctuation">.</span><span class="token class-name">Reader</span> reader <span class="token operator">=</span> <span class="token keyword">null</span><span class="token punctuation">;</span>
        <span class="token keyword">try</span> <span class="token punctuation">{</span>
            reader <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SequenceFile</span><span class="token punctuation">.</span><span class="token class-name">Reader</span><span class="token punctuation">(</span>fs<span class="token punctuation">,</span> path<span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token class-name">Writable</span> key <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token class-name">Writable</span><span class="token punctuation">)</span> <span class="token class-name">ReflectionUtils</span><span class="token punctuation">.</span><span class="token function">newInstance</span><span class="token punctuation">(</span>reader<span class="token punctuation">.</span><span class="token function">getKeyClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token class-name">Writable</span> value <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token class-name">Writable</span><span class="token punctuation">)</span> <span class="token class-name">ReflectionUtils</span><span class="token punctuation">.</span><span class="token function">newInstance</span><span class="token punctuation">(</span>reader<span class="token punctuation">.</span><span class="token function">getValueClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">long</span> position <span class="token operator">=</span> reader<span class="token punctuation">.</span><span class="token function">getPosition</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token keyword">while</span> <span class="token punctuation">(</span>reader<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
                <span class="token class-name">String</span> syncSeen <span class="token operator">=</span> reader<span class="token punctuation">.</span><span class="token function">syncSeen</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token string">&quot;*&quot;</span> <span class="token operator">:</span> <span class="token string">&quot;&quot;</span><span class="token punctuation">;</span>
                <span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;[%s%s]\t%s\t%s\n&quot;</span><span class="token punctuation">,</span>position<span class="token punctuation">,</span>syncSeen<span class="token punctuation">,</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span> <span class="token keyword">finally</span> <span class="token punctuation">{</span>
            <span class="token class-name">IOUtils</span><span class="token punctuation">.</span><span class="token function">closeStream</span><span class="token punctuation">(</span>reader<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>该程序的另一个特性是能够显示顺序文件中同步点的位置信息。所谓同步点，是指数据读取迷路(lost)后能够再一次与记录边界同步的数据流中的某个位置，例如，在数据流中由于搜索而跑到任意位置后可采取此动作。同步点是由SequenceFile.Writer记录的，后者在顺序文件写入过程中插入一个特殊项以便每隔几个记录边有一个同步表示。这样的特殊项非常小，因而只能造成很小的存储开销，不到1%。同步点始终位于记录的边界处。</p> <p>部分运行结果如下图所示：</p> <p><img src="/images/hadoop/SequenceFileReadDemo%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C.jpg" alt="SequenceFileReadDemo运行结果.jpg"></p> <p>在顺序文件中搜索给定位置有两种方法。第一种是调用seek()方法，该方法将指针指向文件中指定的位置。例如，可以按如下方式搜索记录边界：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>reader<span class="token punctuation">.</span><span class="token function">seek</span><span class="token punctuation">(</span><span class="token number">359</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span>reader<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token class-name">IntWritable</span><span class="token punctuation">)</span>key<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token number">95</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>但如果给定位置不是记录边界，调用next()方法时就会报错：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>reader<span class="token punctuation">.</span><span class="token function">seek</span><span class="token punctuation">(</span><span class="token number">360</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
reader<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//fails with IOException</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>第二种方法通过同步点查找记录边界。SequenceFile.Reader对象的sync(long postion)方法可以将读取位置定位到position之后的下一个同步点。如果position之后没有同步了，那么当前读取位置将指向文件末尾。这样，我们对数据流中的任意位置调用sync()方法（不一定时一个记录的边界）而且可以重新定位到下一个同步点并继续向后读取：</p> <div class="language-java line-numbers-mode"><pre class="language-java"><code>reader<span class="token punctuation">.</span><span class="token function">sync</span><span class="token punctuation">(</span><span class="token number">360</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span>reader<span class="token punctuation">.</span><span class="token function">getPosition</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token number">2021L</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span>reader<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">assertThat</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token class-name">IntWritable</span><span class="token punctuation">)</span>key<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">is</span><span class="token punctuation">(</span><span class="token number">59</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><em>SequenceFile.Writer对象有一个sync()方法，该方法可以在数据流的当前位置插入一个同步点。不要把它和Syncable接口中定义的hsync()方法混为一谈，后者用于底层设备缓冲区的同步</em></p> <p>可以将加入同步点的顺序文件作为MapReduce的输入，因为该类顺序文件允许切分，由此该文件的不同部分可以由独立的map任务单独处理。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/hadoop/hdfs.html" class="prev">
        HDFS
      </a></span> <span class="next"><a href="/hadoop/mapreduce.html">
        MapReduce
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.7696aff8.js" defer></script><script src="/assets/js/2.3c301967.js" defer></script><script src="/assets/js/18.a7d27d96.js" defer></script>
  </body>
</html>
